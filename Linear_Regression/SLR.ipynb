{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Reservations_and_Pizzas.csv')\n",
    "reservations = df['Reservations'].to_numpy(dtype='float64')\n",
    "pizzas = df['Pizzas'].to_numpy(dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  2.,  3.,  5.,  6.,  6.,  7.,  8.,  9., 10., 10., 10.,\n",
       "       12., 12., 13., 13., 14., 15., 15., 18., 18., 21., 21., 22., 23.,\n",
       "       26., 26., 27.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16., 13., 16., 15., 17., 16., 18., 22., 23., 26., 17., 21., 27.,\n",
       "       25., 27., 23., 33., 32., 30., 39., 34., 37., 32., 37., 37., 51.,\n",
       "       29., 34., 44.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pizzas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Reservations', fontsize=16)\n",
    "plt.ylabel('Pizzas', fontsize=16)\n",
    "plt.axis([0,50,0,50])\n",
    "plt.plot(reservations, pizzas, \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(reservations, m, b):\n",
    "    return reservations * m + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing that prediction works\n",
    "# based in example parameters\n",
    "predict(14, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The term \"Broadcasting\" is an stardard in numpy and it refers to the \n",
    "### ability of work with arrays of different shapes in arithmetic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.8, 18. , 20.4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservationsBroadcast = np.array([14,5,7])\n",
    "predictions = predict(reservationsBroadcast, 1.2, 12)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, m, b):\n",
    "    predictions = predict(x, m, b)\n",
    "    return np.average((predictions - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.420689655172428"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(reservations, pizzas, 1.2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, iterations, learningRate):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(x, y, w, b)\n",
    "        print('Iteration %4d => Loss %.6f' % (i, current_loss))\n",
    "        if loss(x, y, w - learningRate, b) < current_loss:\n",
    "            w -= learningRate\n",
    "        if loss(x, y, w + learningRate, b) < current_loss:\n",
    "            w += learningRate\n",
    "        if loss(x, y, w, b - learningRate) < current_loss:\n",
    "            b -= learningRate\n",
    "        if loss(x, y, w, b + learningRate) < current_loss:\n",
    "            b += learningRate\n",
    "        else:\n",
    "            return w, b\n",
    "    raise Exception('Couldn\\'t find a result withing %d iterations' % iterations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss 833.137931\n",
      "Iteration    1 => Loss 824.881052\n",
      "Iteration    2 => Loss 816.669034\n",
      "Iteration    3 => Loss 808.501879\n",
      "Iteration    4 => Loss 800.379586\n",
      "Iteration    5 => Loss 792.302155\n",
      "Iteration    6 => Loss 784.269586\n",
      "Iteration    7 => Loss 776.281879\n",
      "Iteration    8 => Loss 768.339034\n",
      "Iteration    9 => Loss 760.441052\n",
      "Iteration   10 => Loss 752.587931\n",
      "Iteration   11 => Loss 744.779672\n",
      "Iteration   12 => Loss 737.016276\n",
      "Iteration   13 => Loss 729.297741\n",
      "Iteration   14 => Loss 721.624069\n",
      "Iteration   15 => Loss 713.995259\n",
      "Iteration   16 => Loss 706.411310\n",
      "Iteration   17 => Loss 698.872224\n",
      "Iteration   18 => Loss 691.378000\n",
      "Iteration   19 => Loss 683.928638\n",
      "Iteration   20 => Loss 676.524138\n",
      "Iteration   21 => Loss 669.164500\n",
      "Iteration   22 => Loss 661.849724\n",
      "Iteration   23 => Loss 654.579810\n",
      "Iteration   24 => Loss 647.354759\n",
      "Iteration   25 => Loss 640.174569\n",
      "Iteration   26 => Loss 633.039241\n",
      "Iteration   27 => Loss 625.948776\n",
      "Iteration   28 => Loss 618.903172\n",
      "Iteration   29 => Loss 611.902431\n",
      "Iteration   30 => Loss 604.946552\n",
      "Iteration   31 => Loss 598.035534\n",
      "Iteration   32 => Loss 591.169379\n",
      "Iteration   33 => Loss 584.348086\n",
      "Iteration   34 => Loss 577.571655\n",
      "Iteration   35 => Loss 570.840086\n",
      "Iteration   36 => Loss 564.153379\n",
      "Iteration   37 => Loss 557.511534\n",
      "Iteration   38 => Loss 550.914552\n",
      "Iteration   39 => Loss 544.362431\n",
      "Iteration   40 => Loss 537.855172\n",
      "Iteration   41 => Loss 531.392776\n",
      "Iteration   42 => Loss 524.975241\n",
      "Iteration   43 => Loss 518.602569\n",
      "Iteration   44 => Loss 512.274759\n",
      "Iteration   45 => Loss 505.991810\n",
      "Iteration   46 => Loss 499.753724\n",
      "Iteration   47 => Loss 493.560500\n",
      "Iteration   48 => Loss 487.412138\n",
      "Iteration   49 => Loss 481.308638\n",
      "Iteration   50 => Loss 475.250000\n",
      "Iteration   51 => Loss 469.236224\n",
      "Iteration   52 => Loss 463.267310\n",
      "Iteration   53 => Loss 457.343259\n",
      "Iteration   54 => Loss 451.464069\n",
      "Iteration   55 => Loss 445.629741\n",
      "Iteration   56 => Loss 439.840276\n",
      "Iteration   57 => Loss 434.095672\n",
      "Iteration   58 => Loss 428.395931\n",
      "Iteration   59 => Loss 422.741052\n",
      "Iteration   60 => Loss 417.131034\n",
      "Iteration   61 => Loss 411.565879\n",
      "Iteration   62 => Loss 406.045586\n",
      "Iteration   63 => Loss 400.570155\n",
      "Iteration   64 => Loss 395.139586\n",
      "Iteration   65 => Loss 389.753879\n",
      "Iteration   66 => Loss 384.413034\n",
      "Iteration   67 => Loss 379.117052\n",
      "Iteration   68 => Loss 373.865931\n",
      "Iteration   69 => Loss 368.659672\n",
      "Iteration   70 => Loss 363.498276\n",
      "Iteration   71 => Loss 358.381741\n",
      "Iteration   72 => Loss 353.310069\n",
      "Iteration   73 => Loss 348.283259\n",
      "Iteration   74 => Loss 343.301310\n",
      "Iteration   75 => Loss 338.364224\n",
      "Iteration   76 => Loss 333.472000\n",
      "Iteration   77 => Loss 328.624638\n",
      "Iteration   78 => Loss 323.822138\n",
      "Iteration   79 => Loss 319.064500\n",
      "Iteration   80 => Loss 314.351724\n",
      "Iteration   81 => Loss 309.683810\n",
      "Iteration   82 => Loss 305.060759\n",
      "Iteration   83 => Loss 300.482569\n",
      "Iteration   84 => Loss 295.949241\n",
      "Iteration   85 => Loss 291.460776\n",
      "Iteration   86 => Loss 287.017172\n",
      "Iteration   87 => Loss 282.618431\n",
      "Iteration   88 => Loss 278.264552\n",
      "Iteration   89 => Loss 273.955534\n",
      "Iteration   90 => Loss 269.691379\n",
      "Iteration   91 => Loss 265.472086\n",
      "Iteration   92 => Loss 261.297655\n",
      "Iteration   93 => Loss 257.168086\n",
      "Iteration   94 => Loss 253.083379\n",
      "Iteration   95 => Loss 249.043534\n",
      "Iteration   96 => Loss 245.048552\n",
      "Iteration   97 => Loss 241.098431\n",
      "Iteration   98 => Loss 237.193172\n",
      "Iteration   99 => Loss 233.332776\n",
      "Iteration  100 => Loss 229.517241\n",
      "Iteration  101 => Loss 225.746569\n",
      "Iteration  102 => Loss 222.020759\n",
      "Iteration  103 => Loss 218.339810\n",
      "Iteration  104 => Loss 214.703724\n",
      "Iteration  105 => Loss 211.112500\n",
      "Iteration  106 => Loss 207.566138\n",
      "Iteration  107 => Loss 204.064638\n",
      "Iteration  108 => Loss 200.608000\n",
      "Iteration  109 => Loss 197.196224\n",
      "Iteration  110 => Loss 193.829310\n",
      "Iteration  111 => Loss 190.507259\n",
      "Iteration  112 => Loss 187.230069\n",
      "Iteration  113 => Loss 183.997741\n",
      "Iteration  114 => Loss 180.810276\n",
      "Iteration  115 => Loss 177.667672\n",
      "Iteration  116 => Loss 174.569931\n",
      "Iteration  117 => Loss 171.517052\n",
      "Iteration  118 => Loss 168.509034\n",
      "Iteration  119 => Loss 165.545879\n",
      "Iteration  120 => Loss 162.627586\n",
      "Iteration  121 => Loss 159.754155\n",
      "Iteration  122 => Loss 156.925586\n",
      "Iteration  123 => Loss 154.141879\n",
      "Iteration  124 => Loss 151.403034\n",
      "Iteration  125 => Loss 148.709052\n",
      "Iteration  126 => Loss 146.059931\n",
      "Iteration  127 => Loss 143.455672\n",
      "Iteration  128 => Loss 140.896276\n",
      "Iteration  129 => Loss 138.381741\n",
      "Iteration  130 => Loss 135.912069\n",
      "Iteration  131 => Loss 133.487259\n",
      "Iteration  132 => Loss 131.107310\n",
      "Iteration  133 => Loss 128.772224\n",
      "Iteration  134 => Loss 126.482000\n",
      "Iteration  135 => Loss 124.236638\n",
      "Iteration  136 => Loss 122.036138\n",
      "Iteration  137 => Loss 119.880500\n",
      "Iteration  138 => Loss 117.769724\n",
      "Iteration  139 => Loss 115.703810\n",
      "Iteration  140 => Loss 113.682759\n",
      "Iteration  141 => Loss 111.706569\n",
      "Iteration  142 => Loss 109.775241\n",
      "Iteration  143 => Loss 107.888776\n",
      "Iteration  144 => Loss 106.047172\n",
      "Iteration  145 => Loss 104.250431\n",
      "Iteration  146 => Loss 102.498552\n",
      "Iteration  147 => Loss 100.791534\n",
      "Iteration  148 => Loss 99.129379\n",
      "Iteration  149 => Loss 97.512086\n",
      "Iteration  150 => Loss 95.939655\n",
      "Iteration  151 => Loss 94.412086\n",
      "Iteration  152 => Loss 92.929379\n",
      "Iteration  153 => Loss 91.491534\n",
      "Iteration  154 => Loss 90.098552\n",
      "Iteration  155 => Loss 88.750431\n",
      "Iteration  156 => Loss 87.447172\n",
      "Iteration  157 => Loss 86.188776\n",
      "Iteration  158 => Loss 84.975241\n",
      "Iteration  159 => Loss 83.806569\n",
      "Iteration  160 => Loss 82.682759\n",
      "Iteration  161 => Loss 81.603810\n",
      "Iteration  162 => Loss 80.569724\n",
      "Iteration  163 => Loss 79.580500\n",
      "Iteration  164 => Loss 78.636138\n",
      "Iteration  165 => Loss 77.736638\n",
      "Iteration  166 => Loss 76.882000\n",
      "Iteration  167 => Loss 76.072224\n",
      "Iteration  168 => Loss 75.307310\n",
      "Iteration  169 => Loss 74.587259\n",
      "Iteration  170 => Loss 73.912069\n",
      "Iteration  171 => Loss 73.281741\n",
      "Iteration  172 => Loss 72.696276\n",
      "Iteration  173 => Loss 72.155672\n",
      "Iteration  174 => Loss 71.659931\n",
      "Iteration  175 => Loss 71.209052\n",
      "Iteration  176 => Loss 70.803034\n",
      "Iteration  177 => Loss 70.441879\n",
      "Iteration  178 => Loss 70.125586\n",
      "Iteration  179 => Loss 69.854155\n",
      "Iteration  180 => Loss 69.627586\n",
      "Iteration  181 => Loss 69.445879\n",
      "Iteration  182 => Loss 69.309034\n",
      "Iteration  183 => Loss 69.217052\n",
      "Iteration  184 => Loss 69.100376\n",
      "Iteration  185 => Loss 69.031021\n",
      "Iteration  186 => Loss 68.961866\n",
      "Iteration  187 => Loss 68.892910\n",
      "Iteration  188 => Loss 68.824155\n",
      "Iteration  189 => Loss 68.755600\n",
      "Iteration  190 => Loss 68.687245\n",
      "Iteration  191 => Loss 68.619090\n",
      "Iteration  192 => Loss 68.551134\n",
      "Iteration  193 => Loss 68.483379\n",
      "Iteration  194 => Loss 68.415824\n",
      "Iteration  195 => Loss 68.348469\n",
      "Iteration  196 => Loss 68.281314\n",
      "Iteration  197 => Loss 68.214359\n",
      "Iteration  198 => Loss 68.147603\n",
      "Iteration  199 => Loss 68.081048\n",
      "Iteration  200 => Loss 68.014693\n",
      "Iteration  201 => Loss 67.948538\n",
      "Iteration  202 => Loss 67.882583\n",
      "Iteration  203 => Loss 67.812224\n",
      "Iteration  204 => Loss 67.744083\n",
      "Iteration  205 => Loss 67.676141\n",
      "Iteration  206 => Loss 67.608400\n",
      "Iteration  207 => Loss 67.540859\n",
      "Iteration  208 => Loss 67.473517\n",
      "Iteration  209 => Loss 67.406376\n",
      "Iteration  210 => Loss 67.339434\n",
      "Iteration  211 => Loss 67.272693\n",
      "Iteration  212 => Loss 67.206152\n",
      "Iteration  213 => Loss 67.139810\n",
      "Iteration  214 => Loss 67.073669\n",
      "Iteration  215 => Loss 67.007728\n",
      "Iteration  216 => Loss 66.941986\n",
      "Iteration  217 => Loss 66.876445\n",
      "Iteration  218 => Loss 66.811103\n",
      "Iteration  219 => Loss 66.745962\n",
      "Iteration  220 => Loss 66.677314\n",
      "Iteration  221 => Loss 66.609986\n",
      "Iteration  222 => Loss 66.542859\n",
      "Iteration  223 => Loss 66.475931\n",
      "Iteration  224 => Loss 66.409203\n",
      "Iteration  225 => Loss 66.342676\n",
      "Iteration  226 => Loss 66.276348\n",
      "Iteration  227 => Loss 66.210221\n",
      "Iteration  228 => Loss 66.144293\n",
      "Iteration  229 => Loss 66.078566\n",
      "Iteration  230 => Loss 66.013038\n",
      "Iteration  231 => Loss 65.947710\n",
      "Iteration  232 => Loss 65.882583\n",
      "Iteration  233 => Loss 65.817655\n",
      "Iteration  234 => Loss 65.752928\n",
      "Iteration  235 => Loss 65.688400\n",
      "Iteration  236 => Loss 65.624072\n",
      "Iteration  237 => Loss 65.557134\n",
      "Iteration  238 => Loss 65.490621\n",
      "Iteration  239 => Loss 65.424307\n",
      "Iteration  240 => Loss 65.358193\n",
      "Iteration  241 => Loss 65.292279\n",
      "Iteration  242 => Loss 65.226566\n",
      "Iteration  243 => Loss 65.161052\n",
      "Iteration  244 => Loss 65.095738\n",
      "Iteration  245 => Loss 65.030624\n",
      "Iteration  246 => Loss 64.965710\n",
      "Iteration  247 => Loss 64.900997\n",
      "Iteration  248 => Loss 64.836483\n",
      "Iteration  249 => Loss 64.772169\n",
      "Iteration  250 => Loss 64.708055\n",
      "Iteration  251 => Loss 64.644141\n",
      "Iteration  252 => Loss 64.580428\n",
      "Iteration  253 => Loss 64.516914\n",
      "Iteration  254 => Loss 64.453600\n",
      "Iteration  255 => Loss 64.385986\n",
      "Iteration  256 => Loss 64.320486\n",
      "Iteration  257 => Loss 64.255186\n",
      "Iteration  258 => Loss 64.190086\n",
      "Iteration  259 => Loss 64.125186\n",
      "Iteration  260 => Loss 64.060486\n",
      "Iteration  261 => Loss 63.995986\n",
      "Iteration  262 => Loss 63.931686\n",
      "Iteration  263 => Loss 63.867586\n",
      "Iteration  264 => Loss 63.803686\n",
      "Iteration  265 => Loss 63.739986\n",
      "Iteration  266 => Loss 63.676486\n",
      "Iteration  267 => Loss 63.613186\n",
      "Iteration  268 => Loss 63.550086\n",
      "Iteration  269 => Loss 63.487186\n",
      "Iteration  270 => Loss 63.424486\n",
      "Iteration  271 => Loss 63.361986\n",
      "Iteration  272 => Loss 63.296083\n",
      "Iteration  273 => Loss 63.231397\n",
      "Iteration  274 => Loss 63.166910\n",
      "Iteration  275 => Loss 63.102624\n",
      "Iteration  276 => Loss 63.038538\n",
      "Iteration  277 => Loss 62.974652\n",
      "Iteration  278 => Loss 62.910966\n",
      "Iteration  279 => Loss 62.847479\n",
      "Iteration  280 => Loss 62.784193\n",
      "Iteration  281 => Loss 62.721107\n",
      "Iteration  282 => Loss 62.658221\n",
      "Iteration  283 => Loss 62.595534\n",
      "Iteration  284 => Loss 62.533048\n",
      "Iteration  285 => Loss 62.470762\n",
      "Iteration  286 => Loss 62.408676\n",
      "Iteration  287 => Loss 62.346790\n",
      "Iteration  288 => Loss 62.285103\n",
      "Iteration  289 => Loss 62.220910\n",
      "Iteration  290 => Loss 62.157038\n",
      "Iteration  291 => Loss 62.093366\n",
      "Iteration  292 => Loss 62.029893\n",
      "Iteration  293 => Loss 61.966621\n",
      "Iteration  294 => Loss 61.903548\n",
      "Iteration  295 => Loss 61.840676\n",
      "Iteration  296 => Loss 61.778003\n",
      "Iteration  297 => Loss 61.715531\n",
      "Iteration  298 => Loss 61.653259\n",
      "Iteration  299 => Loss 61.591186\n",
      "Iteration  300 => Loss 61.529314\n",
      "Iteration  301 => Loss 61.467641\n",
      "Iteration  302 => Loss 61.406169\n",
      "Iteration  303 => Loss 61.344897\n",
      "Iteration  304 => Loss 61.283824\n",
      "Iteration  305 => Loss 61.222952\n",
      "Iteration  306 => Loss 61.162279\n",
      "Iteration  307 => Loss 61.097410\n",
      "Iteration  308 => Loss 61.034552\n",
      "Iteration  309 => Loss 60.971893\n",
      "Iteration  310 => Loss 60.909434\n",
      "Iteration  311 => Loss 60.847176\n",
      "Iteration  312 => Loss 60.785117\n",
      "Iteration  313 => Loss 60.723259\n",
      "Iteration  314 => Loss 60.661600\n",
      "Iteration  315 => Loss 60.600141\n",
      "Iteration  316 => Loss 60.538883\n",
      "Iteration  317 => Loss 60.477824\n",
      "Iteration  318 => Loss 60.416966\n",
      "Iteration  319 => Loss 60.356307\n",
      "Iteration  320 => Loss 60.295848\n",
      "Iteration  321 => Loss 60.235590\n",
      "Iteration  322 => Loss 60.175531\n",
      "Iteration  323 => Loss 60.115672\n",
      "Iteration  324 => Loss 60.052514\n",
      "Iteration  325 => Loss 59.990469\n",
      "Iteration  326 => Loss 59.928624\n",
      "Iteration  327 => Loss 59.866979\n",
      "Iteration  328 => Loss 59.805534\n",
      "Iteration  329 => Loss 59.744290\n",
      "Iteration  330 => Loss 59.683245\n",
      "Iteration  331 => Loss 59.622400\n",
      "Iteration  332 => Loss 59.561755\n",
      "Iteration  333 => Loss 59.501310\n",
      "Iteration  334 => Loss 59.441066\n",
      "Iteration  335 => Loss 59.381021\n",
      "Iteration  336 => Loss 59.321176\n",
      "Iteration  337 => Loss 59.261531\n",
      "Iteration  338 => Loss 59.202086\n",
      "Iteration  339 => Loss 59.142841\n",
      "Iteration  340 => Loss 59.083797\n",
      "Iteration  341 => Loss 59.022348\n",
      "Iteration  342 => Loss 58.961117\n",
      "Iteration  343 => Loss 58.900086\n",
      "Iteration  344 => Loss 58.839255\n",
      "Iteration  345 => Loss 58.778624\n",
      "Iteration  346 => Loss 58.718193\n",
      "Iteration  347 => Loss 58.657962\n",
      "Iteration  348 => Loss 58.597931\n",
      "Iteration  349 => Loss 58.538100\n",
      "Iteration  350 => Loss 58.478469\n",
      "Iteration  351 => Loss 58.419038\n",
      "Iteration  352 => Loss 58.359807\n",
      "Iteration  353 => Loss 58.300776\n",
      "Iteration  354 => Loss 58.241945\n",
      "Iteration  355 => Loss 58.183314\n",
      "Iteration  356 => Loss 58.124883\n",
      "Iteration  357 => Loss 58.066652\n",
      "Iteration  358 => Loss 58.008621\n",
      "Iteration  359 => Loss 57.946497\n",
      "Iteration  360 => Loss 57.886279\n",
      "Iteration  361 => Loss 57.826262\n",
      "Iteration  362 => Loss 57.766445\n",
      "Iteration  363 => Loss 57.706828\n",
      "Iteration  364 => Loss 57.647410\n",
      "Iteration  365 => Loss 57.588193\n",
      "Iteration  366 => Loss 57.529176\n",
      "Iteration  367 => Loss 57.470359\n",
      "Iteration  368 => Loss 57.411741\n",
      "Iteration  369 => Loss 57.353324\n",
      "Iteration  370 => Loss 57.295107\n",
      "Iteration  371 => Loss 57.237090\n",
      "Iteration  372 => Loss 57.179272\n",
      "Iteration  373 => Loss 57.121655\n",
      "Iteration  374 => Loss 57.064238\n",
      "Iteration  375 => Loss 57.007021\n",
      "Iteration  376 => Loss 56.946607\n",
      "Iteration  377 => Loss 56.887203\n",
      "Iteration  378 => Loss 56.828000\n",
      "Iteration  379 => Loss 56.768997\n",
      "Iteration  380 => Loss 56.710193\n",
      "Iteration  381 => Loss 56.651590\n",
      "Iteration  382 => Loss 56.593186\n",
      "Iteration  383 => Loss 56.534983\n",
      "Iteration  384 => Loss 56.476979\n",
      "Iteration  385 => Loss 56.419176\n",
      "Iteration  386 => Loss 56.361572\n",
      "Iteration  387 => Loss 56.304169\n",
      "Iteration  388 => Loss 56.246966\n",
      "Iteration  389 => Loss 56.189962\n",
      "Iteration  390 => Loss 56.133159\n",
      "Iteration  391 => Loss 56.076555\n",
      "Iteration  392 => Loss 56.020152\n",
      "Iteration  393 => Loss 55.963948\n",
      "Iteration  394 => Loss 55.902859\n",
      "Iteration  395 => Loss 55.844469\n",
      "Iteration  396 => Loss 55.786279\n",
      "Iteration  397 => Loss 55.728290\n",
      "Iteration  398 => Loss 55.670500\n",
      "Iteration  399 => Loss 55.612910\n",
      "Iteration  400 => Loss 55.555521\n",
      "Iteration  401 => Loss 55.498331\n",
      "Iteration  402 => Loss 55.441341\n",
      "Iteration  403 => Loss 55.384552\n",
      "Iteration  404 => Loss 55.327962\n",
      "Iteration  405 => Loss 55.271572\n",
      "Iteration  406 => Loss 55.215383\n",
      "Iteration  407 => Loss 55.159393\n",
      "Iteration  408 => Loss 55.103603\n",
      "Iteration  409 => Loss 55.048014\n",
      "Iteration  410 => Loss 54.992624\n",
      "Iteration  411 => Loss 54.933245\n",
      "Iteration  412 => Loss 54.875669\n",
      "Iteration  413 => Loss 54.818293\n",
      "Iteration  414 => Loss 54.761117\n",
      "Iteration  415 => Loss 54.704141\n",
      "Iteration  416 => Loss 54.647366\n",
      "Iteration  417 => Loss 54.590790\n",
      "Iteration  418 => Loss 54.534414\n",
      "Iteration  419 => Loss 54.478238\n",
      "Iteration  420 => Loss 54.422262\n",
      "Iteration  421 => Loss 54.366486\n",
      "Iteration  422 => Loss 54.310910\n",
      "Iteration  423 => Loss 54.255534\n",
      "Iteration  424 => Loss 54.200359\n",
      "Iteration  425 => Loss 54.145383\n",
      "Iteration  426 => Loss 54.090607\n",
      "Iteration  427 => Loss 54.036031\n",
      "Iteration  428 => Loss 53.978362\n",
      "Iteration  429 => Loss 53.921600\n",
      "Iteration  430 => Loss 53.865038\n",
      "Iteration  431 => Loss 53.808676\n",
      "Iteration  432 => Loss 53.752514\n",
      "Iteration  433 => Loss 53.696552\n",
      "Iteration  434 => Loss 53.640790\n",
      "Iteration  435 => Loss 53.585228\n",
      "Iteration  436 => Loss 53.529866\n",
      "Iteration  437 => Loss 53.474703\n",
      "Iteration  438 => Loss 53.419741\n",
      "Iteration  439 => Loss 53.364979\n",
      "Iteration  440 => Loss 53.310417\n",
      "Iteration  441 => Loss 53.256055\n",
      "Iteration  442 => Loss 53.201893\n",
      "Iteration  443 => Loss 53.147931\n",
      "Iteration  444 => Loss 53.094169\n",
      "Iteration  445 => Loss 53.040607\n",
      "Iteration  446 => Loss 52.982262\n",
      "Iteration  447 => Loss 52.926514\n",
      "Iteration  448 => Loss 52.870966\n",
      "Iteration  449 => Loss 52.815617\n",
      "Iteration  450 => Loss 52.760469\n",
      "Iteration  451 => Loss 52.705521\n",
      "Iteration  452 => Loss 52.650772\n",
      "Iteration  453 => Loss 52.596224\n",
      "Iteration  454 => Loss 52.541876\n",
      "Iteration  455 => Loss 52.487728\n",
      "Iteration  456 => Loss 52.433779\n",
      "Iteration  457 => Loss 52.380031\n",
      "Iteration  458 => Loss 52.326483\n",
      "Iteration  459 => Loss 52.273134\n",
      "Iteration  460 => Loss 52.219986\n",
      "Iteration  461 => Loss 52.167038\n",
      "Iteration  462 => Loss 52.114290\n",
      "Iteration  463 => Loss 52.057655\n",
      "Iteration  464 => Loss 52.002721\n",
      "Iteration  465 => Loss 51.947986\n",
      "Iteration  466 => Loss 51.893452\n",
      "Iteration  467 => Loss 51.839117\n",
      "Iteration  468 => Loss 51.784983\n",
      "Iteration  469 => Loss 51.731048\n",
      "Iteration  470 => Loss 51.677314\n",
      "Iteration  471 => Loss 51.623779\n",
      "Iteration  472 => Loss 51.570445\n",
      "Iteration  473 => Loss 51.517310\n",
      "Iteration  474 => Loss 51.464376\n",
      "Iteration  475 => Loss 51.411641\n",
      "Iteration  476 => Loss 51.359107\n",
      "Iteration  477 => Loss 51.306772\n",
      "Iteration  478 => Loss 51.254638\n",
      "Iteration  479 => Loss 51.202703\n",
      "Iteration  480 => Loss 51.147779\n",
      "Iteration  481 => Loss 51.093659\n",
      "Iteration  482 => Loss 51.039738\n",
      "Iteration  483 => Loss 50.986017\n",
      "Iteration  484 => Loss 50.932497\n",
      "Iteration  485 => Loss 50.879176\n",
      "Iteration  486 => Loss 50.826055\n",
      "Iteration  487 => Loss 50.773134\n",
      "Iteration  488 => Loss 50.720414\n",
      "Iteration  489 => Loss 50.667893\n",
      "Iteration  490 => Loss 50.615572\n",
      "Iteration  491 => Loss 50.563452\n",
      "Iteration  492 => Loss 50.511531\n",
      "Iteration  493 => Loss 50.459810\n",
      "Iteration  494 => Loss 50.408290\n",
      "Iteration  495 => Loss 50.356969\n",
      "Iteration  496 => Loss 50.305848\n",
      "Iteration  497 => Loss 50.254928\n",
      "Iteration  498 => Loss 50.199328\n",
      "Iteration  499 => Loss 50.146221\n",
      "Iteration  500 => Loss 50.093314\n",
      "Iteration  501 => Loss 50.040607\n",
      "Iteration  502 => Loss 49.988100\n",
      "Iteration  503 => Loss 49.935793\n",
      "Iteration  504 => Loss 49.883686\n",
      "Iteration  505 => Loss 49.831779\n",
      "Iteration  506 => Loss 49.780072\n",
      "Iteration  507 => Loss 49.728566\n",
      "Iteration  508 => Loss 49.677259\n",
      "Iteration  509 => Loss 49.626152\n",
      "Iteration  510 => Loss 49.575245\n",
      "Iteration  511 => Loss 49.524538\n",
      "Iteration  512 => Loss 49.474031\n",
      "Iteration  513 => Loss 49.423724\n",
      "Iteration  514 => Loss 49.373617\n",
      "Iteration  515 => Loss 49.319728\n",
      "Iteration  516 => Loss 49.267434\n",
      "Iteration  517 => Loss 49.215341\n",
      "Iteration  518 => Loss 49.163448\n",
      "Iteration  519 => Loss 49.111755\n",
      "Iteration  520 => Loss 49.060262\n",
      "Iteration  521 => Loss 49.008969\n",
      "Iteration  522 => Loss 48.957876\n",
      "Iteration  523 => Loss 48.906983\n",
      "Iteration  524 => Loss 48.856290\n",
      "Iteration  525 => Loss 48.805797\n",
      "Iteration  526 => Loss 48.755503\n",
      "Iteration  527 => Loss 48.705410\n",
      "Iteration  528 => Loss 48.655517\n",
      "Iteration  529 => Loss 48.605824\n",
      "Iteration  530 => Loss 48.556331\n",
      "Iteration  531 => Loss 48.507038\n",
      "Iteration  532 => Loss 48.454859\n",
      "Iteration  533 => Loss 48.403379\n",
      "Iteration  534 => Loss 48.352100\n",
      "Iteration  535 => Loss 48.301021\n",
      "Iteration  536 => Loss 48.250141\n",
      "Iteration  537 => Loss 48.199462\n",
      "Iteration  538 => Loss 48.148983\n",
      "Iteration  539 => Loss 48.098703\n",
      "Iteration  540 => Loss 48.048624\n",
      "Iteration  541 => Loss 47.998745\n",
      "Iteration  542 => Loss 47.949066\n",
      "Iteration  543 => Loss 47.899586\n",
      "Iteration  544 => Loss 47.850307\n",
      "Iteration  545 => Loss 47.801228\n",
      "Iteration  546 => Loss 47.752348\n",
      "Iteration  547 => Loss 47.703669\n",
      "Iteration  548 => Loss 47.655190\n",
      "Iteration  549 => Loss 47.606910\n",
      "Iteration  550 => Loss 47.554055\n",
      "Iteration  551 => Loss 47.503590\n",
      "Iteration  552 => Loss 47.453324\n",
      "Iteration  553 => Loss 47.403259\n",
      "Iteration  554 => Loss 47.353393\n",
      "Iteration  555 => Loss 47.303728\n",
      "Iteration  556 => Loss 47.254262\n",
      "Iteration  557 => Loss 47.204997\n",
      "Iteration  558 => Loss 47.155931\n",
      "Iteration  559 => Loss 47.107066\n",
      "Iteration  560 => Loss 47.058400\n",
      "Iteration  561 => Loss 47.009934\n",
      "Iteration  562 => Loss 46.961669\n",
      "Iteration  563 => Loss 46.913603\n",
      "Iteration  564 => Loss 46.865738\n",
      "Iteration  565 => Loss 46.818072\n",
      "Iteration  566 => Loss 46.770607\n",
      "Iteration  567 => Loss 46.719462\n",
      "Iteration  568 => Loss 46.669810\n",
      "Iteration  569 => Loss 46.620359\n",
      "Iteration  570 => Loss 46.571107\n",
      "Iteration  571 => Loss 46.522055\n",
      "Iteration  572 => Loss 46.473203\n",
      "Iteration  573 => Loss 46.424552\n",
      "Iteration  574 => Loss 46.376100\n",
      "Iteration  575 => Loss 46.327848\n",
      "Iteration  576 => Loss 46.279797\n",
      "Iteration  577 => Loss 46.231945\n",
      "Iteration  578 => Loss 46.184293\n",
      "Iteration  579 => Loss 46.136841\n",
      "Iteration  580 => Loss 46.089590\n",
      "Iteration  581 => Loss 46.042538\n",
      "Iteration  582 => Loss 45.995686\n",
      "Iteration  583 => Loss 45.949034\n",
      "Iteration  584 => Loss 45.899600\n",
      "Iteration  585 => Loss 45.850762\n",
      "Iteration  586 => Loss 45.802124\n",
      "Iteration  587 => Loss 45.753686\n",
      "Iteration  588 => Loss 45.705448\n",
      "Iteration  589 => Loss 45.657410\n",
      "Iteration  590 => Loss 45.609572\n",
      "Iteration  591 => Loss 45.561934\n",
      "Iteration  592 => Loss 45.514497\n",
      "Iteration  593 => Loss 45.467259\n",
      "Iteration  594 => Loss 45.420221\n",
      "Iteration  595 => Loss 45.373383\n",
      "Iteration  596 => Loss 45.326745\n",
      "Iteration  597 => Loss 45.280307\n",
      "Iteration  598 => Loss 45.234069\n",
      "Iteration  599 => Loss 45.188031\n",
      "Iteration  600 => Loss 45.142193\n",
      "Iteration  601 => Loss 45.096555\n",
      "Iteration  602 => Loss 45.046445\n",
      "Iteration  603 => Loss 44.998621\n",
      "Iteration  604 => Loss 44.950997\n",
      "Iteration  605 => Loss 44.903572\n",
      "Iteration  606 => Loss 44.856348\n",
      "Iteration  607 => Loss 44.809324\n",
      "Iteration  608 => Loss 44.762500\n",
      "Iteration  609 => Loss 44.715876\n",
      "Iteration  610 => Loss 44.669452\n",
      "Iteration  611 => Loss 44.623228\n",
      "Iteration  612 => Loss 44.577203\n",
      "Iteration  613 => Loss 44.531379\n",
      "Iteration  614 => Loss 44.485755\n",
      "Iteration  615 => Loss 44.440331\n",
      "Iteration  616 => Loss 44.395107\n",
      "Iteration  617 => Loss 44.350083\n",
      "Iteration  618 => Loss 44.305259\n",
      "Iteration  619 => Loss 44.256859\n",
      "Iteration  620 => Loss 44.209848\n",
      "Iteration  621 => Loss 44.163038\n",
      "Iteration  622 => Loss 44.116428\n",
      "Iteration  623 => Loss 44.070017\n",
      "Iteration  624 => Loss 44.023807\n",
      "Iteration  625 => Loss 43.977797\n",
      "Iteration  626 => Loss 43.931986\n",
      "Iteration  627 => Loss 43.886376\n",
      "Iteration  628 => Loss 43.840966\n",
      "Iteration  629 => Loss 43.795755\n",
      "Iteration  630 => Loss 43.750745\n",
      "Iteration  631 => Loss 43.705934\n",
      "Iteration  632 => Loss 43.661324\n",
      "Iteration  633 => Loss 43.616914\n",
      "Iteration  634 => Loss 43.572703\n",
      "Iteration  635 => Loss 43.528693\n",
      "Iteration  636 => Loss 43.482003\n",
      "Iteration  637 => Loss 43.435807\n",
      "Iteration  638 => Loss 43.389810\n",
      "Iteration  639 => Loss 43.344014\n",
      "Iteration  640 => Loss 43.298417\n",
      "Iteration  641 => Loss 43.253021\n",
      "Iteration  642 => Loss 43.207824\n",
      "Iteration  643 => Loss 43.162828\n",
      "Iteration  644 => Loss 43.118031\n",
      "Iteration  645 => Loss 43.073434\n",
      "Iteration  646 => Loss 43.029038\n",
      "Iteration  647 => Loss 42.984841\n",
      "Iteration  648 => Loss 42.940845\n",
      "Iteration  649 => Loss 42.897048\n",
      "Iteration  650 => Loss 42.853452\n",
      "Iteration  651 => Loss 42.810055\n",
      "Iteration  652 => Loss 42.766859\n",
      "Iteration  653 => Loss 42.723862\n",
      "Iteration  654 => Loss 42.676497\n",
      "Iteration  655 => Loss 42.631314\n",
      "Iteration  656 => Loss 42.586331\n",
      "Iteration  657 => Loss 42.541548\n",
      "Iteration  658 => Loss 42.496966\n",
      "Iteration  659 => Loss 42.452583\n",
      "Iteration  660 => Loss 42.408400\n",
      "Iteration  661 => Loss 42.364417\n",
      "Iteration  662 => Loss 42.320634\n",
      "Iteration  663 => Loss 42.277052\n",
      "Iteration  664 => Loss 42.233669\n",
      "Iteration  665 => Loss 42.190486\n",
      "Iteration  666 => Loss 42.147503\n",
      "Iteration  667 => Loss 42.104721\n",
      "Iteration  668 => Loss 42.062138\n",
      "Iteration  669 => Loss 42.019755\n",
      "Iteration  670 => Loss 41.977572\n",
      "Iteration  671 => Loss 41.931917\n",
      "Iteration  672 => Loss 41.887548\n",
      "Iteration  673 => Loss 41.843379\n",
      "Iteration  674 => Loss 41.799410\n",
      "Iteration  675 => Loss 41.755641\n",
      "Iteration  676 => Loss 41.712072\n",
      "Iteration  677 => Loss 41.668703\n",
      "Iteration  678 => Loss 41.625534\n",
      "Iteration  679 => Loss 41.582566\n",
      "Iteration  680 => Loss 41.539797\n",
      "Iteration  681 => Loss 41.497228\n",
      "Iteration  682 => Loss 41.454859\n",
      "Iteration  683 => Loss 41.412690\n",
      "Iteration  684 => Loss 41.370721\n",
      "Iteration  685 => Loss 41.328952\n",
      "Iteration  686 => Loss 41.287383\n",
      "Iteration  687 => Loss 41.246014\n",
      "Iteration  688 => Loss 41.202069\n",
      "Iteration  689 => Loss 41.158514\n",
      "Iteration  690 => Loss 41.115159\n",
      "Iteration  691 => Loss 41.072003\n",
      "Iteration  692 => Loss 41.029048\n",
      "Iteration  693 => Loss 40.986293\n",
      "Iteration  694 => Loss 40.943738\n",
      "Iteration  695 => Loss 40.901383\n",
      "Iteration  696 => Loss 40.859228\n",
      "Iteration  697 => Loss 40.817272\n",
      "Iteration  698 => Loss 40.775517\n",
      "Iteration  699 => Loss 40.733962\n",
      "Iteration  700 => Loss 40.692607\n",
      "Iteration  701 => Loss 40.651452\n",
      "Iteration  702 => Loss 40.610497\n",
      "Iteration  703 => Loss 40.569741\n",
      "Iteration  704 => Loss 40.529186\n",
      "Iteration  705 => Loss 40.488831\n",
      "Iteration  706 => Loss 40.444210\n",
      "Iteration  707 => Loss 40.401669\n",
      "Iteration  708 => Loss 40.359328\n",
      "Iteration  709 => Loss 40.317186\n",
      "Iteration  710 => Loss 40.275245\n",
      "Iteration  711 => Loss 40.233503\n",
      "Iteration  712 => Loss 40.191962\n",
      "Iteration  713 => Loss 40.150621\n",
      "Iteration  714 => Loss 40.109479\n",
      "Iteration  715 => Loss 40.068538\n",
      "Iteration  716 => Loss 40.027797\n",
      "Iteration  717 => Loss 39.987255\n",
      "Iteration  718 => Loss 39.946914\n",
      "Iteration  719 => Loss 39.906772\n",
      "Iteration  720 => Loss 39.866831\n",
      "Iteration  721 => Loss 39.827090\n",
      "Iteration  722 => Loss 39.787548\n",
      "Iteration  723 => Loss 39.744638\n",
      "Iteration  724 => Loss 39.702910\n",
      "Iteration  725 => Loss 39.661383\n",
      "Iteration  726 => Loss 39.620055\n",
      "Iteration  727 => Loss 39.578928\n",
      "Iteration  728 => Loss 39.538000\n",
      "Iteration  729 => Loss 39.497272\n",
      "Iteration  730 => Loss 39.456745\n",
      "Iteration  731 => Loss 39.416417\n",
      "Iteration  732 => Loss 39.376290\n",
      "Iteration  733 => Loss 39.336362\n",
      "Iteration  734 => Loss 39.296634\n",
      "Iteration  735 => Loss 39.257107\n",
      "Iteration  736 => Loss 39.217779\n",
      "Iteration  737 => Loss 39.178652\n",
      "Iteration  738 => Loss 39.139724\n",
      "Iteration  739 => Loss 39.100997\n",
      "Iteration  740 => Loss 39.059797\n",
      "Iteration  741 => Loss 39.018883\n",
      "Iteration  742 => Loss 38.978169\n",
      "Iteration  743 => Loss 38.937655\n",
      "Iteration  744 => Loss 38.897341\n",
      "Iteration  745 => Loss 38.857228\n",
      "Iteration  746 => Loss 38.817314\n",
      "Iteration  747 => Loss 38.777600\n",
      "Iteration  748 => Loss 38.738086\n",
      "Iteration  749 => Loss 38.698772\n",
      "Iteration  750 => Loss 38.659659\n",
      "Iteration  751 => Loss 38.620745\n",
      "Iteration  752 => Loss 38.582031\n",
      "Iteration  753 => Loss 38.543517\n",
      "Iteration  754 => Loss 38.505203\n",
      "Iteration  755 => Loss 38.467090\n",
      "Iteration  756 => Loss 38.429176\n",
      "Iteration  757 => Loss 38.391462\n",
      "Iteration  758 => Loss 38.349586\n",
      "Iteration  759 => Loss 38.309686\n",
      "Iteration  760 => Loss 38.269986\n",
      "Iteration  761 => Loss 38.230486\n",
      "Iteration  762 => Loss 38.191186\n",
      "Iteration  763 => Loss 38.152086\n",
      "Iteration  764 => Loss 38.113186\n",
      "Iteration  765 => Loss 38.074486\n",
      "Iteration  766 => Loss 38.035986\n",
      "Iteration  767 => Loss 37.997686\n",
      "Iteration  768 => Loss 37.959586\n",
      "Iteration  769 => Loss 37.921686\n",
      "Iteration  770 => Loss 37.883986\n",
      "Iteration  771 => Loss 37.846486\n",
      "Iteration  772 => Loss 37.809186\n",
      "Iteration  773 => Loss 37.772086\n",
      "Iteration  774 => Loss 37.735186\n",
      "Iteration  775 => Loss 37.695021\n",
      "Iteration  776 => Loss 37.655934\n",
      "Iteration  777 => Loss 37.617048\n",
      "Iteration  778 => Loss 37.578362\n",
      "Iteration  779 => Loss 37.539876\n",
      "Iteration  780 => Loss 37.501590\n",
      "Iteration  781 => Loss 37.463503\n",
      "Iteration  782 => Loss 37.425617\n",
      "Iteration  783 => Loss 37.387931\n",
      "Iteration  784 => Loss 37.350445\n",
      "Iteration  785 => Loss 37.313159\n",
      "Iteration  786 => Loss 37.276072\n",
      "Iteration  787 => Loss 37.239186\n",
      "Iteration  788 => Loss 37.202500\n",
      "Iteration  789 => Loss 37.166014\n",
      "Iteration  790 => Loss 37.129728\n",
      "Iteration  791 => Loss 37.093641\n",
      "Iteration  792 => Loss 37.057755\n",
      "Iteration  793 => Loss 37.016914\n",
      "Iteration  794 => Loss 36.978841\n",
      "Iteration  795 => Loss 36.940969\n",
      "Iteration  796 => Loss 36.903297\n",
      "Iteration  797 => Loss 36.865824\n",
      "Iteration  798 => Loss 36.828552\n",
      "Iteration  799 => Loss 36.791479\n",
      "Iteration  800 => Loss 36.754607\n",
      "Iteration  801 => Loss 36.717934\n",
      "Iteration  802 => Loss 36.681462\n",
      "Iteration  803 => Loss 36.645190\n",
      "Iteration  804 => Loss 36.609117\n",
      "Iteration  805 => Loss 36.573245\n",
      "Iteration  806 => Loss 36.537572\n",
      "Iteration  807 => Loss 36.502100\n",
      "Iteration  808 => Loss 36.466828\n",
      "Iteration  809 => Loss 36.431755\n",
      "Iteration  810 => Loss 36.392624\n",
      "Iteration  811 => Loss 36.355366\n",
      "Iteration  812 => Loss 36.318307\n",
      "Iteration  813 => Loss 36.281448\n",
      "Iteration  814 => Loss 36.244790\n",
      "Iteration  815 => Loss 36.208331\n",
      "Iteration  816 => Loss 36.172072\n",
      "Iteration  817 => Loss 36.136014\n",
      "Iteration  818 => Loss 36.100155\n",
      "Iteration  819 => Loss 36.064497\n",
      "Iteration  820 => Loss 36.029038\n",
      "Iteration  821 => Loss 35.993779\n",
      "Iteration  822 => Loss 35.958721\n",
      "Iteration  823 => Loss 35.923862\n",
      "Iteration  824 => Loss 35.889203\n",
      "Iteration  825 => Loss 35.854745\n",
      "Iteration  826 => Loss 35.820486\n",
      "Iteration  827 => Loss 35.783066\n",
      "Iteration  828 => Loss 35.746621\n",
      "Iteration  829 => Loss 35.710376\n",
      "Iteration  830 => Loss 35.674331\n",
      "Iteration  831 => Loss 35.638486\n",
      "Iteration  832 => Loss 35.602841\n",
      "Iteration  833 => Loss 35.567397\n",
      "Iteration  834 => Loss 35.532152\n",
      "Iteration  835 => Loss 35.497107\n",
      "Iteration  836 => Loss 35.462262\n",
      "Iteration  837 => Loss 35.427617\n",
      "Iteration  838 => Loss 35.393172\n",
      "Iteration  839 => Loss 35.358928\n",
      "Iteration  840 => Loss 35.324883\n",
      "Iteration  841 => Loss 35.291038\n",
      "Iteration  842 => Loss 35.257393\n",
      "Iteration  843 => Loss 35.223948\n",
      "Iteration  844 => Loss 35.190703\n",
      "Iteration  845 => Loss 35.152607\n",
      "Iteration  846 => Loss 35.117176\n",
      "Iteration  847 => Loss 35.081945\n",
      "Iteration  848 => Loss 35.046914\n",
      "Iteration  849 => Loss 35.012083\n",
      "Iteration  850 => Loss 34.977452\n",
      "Iteration  851 => Loss 34.943021\n",
      "Iteration  852 => Loss 34.908790\n",
      "Iteration  853 => Loss 34.874759\n",
      "Iteration  854 => Loss 34.840928\n",
      "Iteration  855 => Loss 34.807297\n",
      "Iteration  856 => Loss 34.773866\n",
      "Iteration  857 => Loss 34.740634\n",
      "Iteration  858 => Loss 34.707603\n",
      "Iteration  859 => Loss 34.674772\n",
      "Iteration  860 => Loss 34.642141\n",
      "Iteration  861 => Loss 34.609710\n",
      "Iteration  862 => Loss 34.573324\n",
      "Iteration  863 => Loss 34.538707\n",
      "Iteration  864 => Loss 34.504290\n",
      "Iteration  865 => Loss 34.470072\n",
      "Iteration  866 => Loss 34.436055\n",
      "Iteration  867 => Loss 34.402238\n",
      "Iteration  868 => Loss 34.368621\n",
      "Iteration  869 => Loss 34.335203\n",
      "Iteration  870 => Loss 34.301986\n",
      "Iteration  871 => Loss 34.268969\n",
      "Iteration  872 => Loss 34.236152\n",
      "Iteration  873 => Loss 34.203534\n",
      "Iteration  874 => Loss 34.171117\n",
      "Iteration  875 => Loss 34.138900\n",
      "Iteration  876 => Loss 34.106883\n",
      "Iteration  877 => Loss 34.075066\n",
      "Iteration  878 => Loss 34.043448\n",
      "Iteration  879 => Loss 34.008772\n",
      "Iteration  880 => Loss 33.974969\n",
      "Iteration  881 => Loss 33.941366\n",
      "Iteration  882 => Loss 33.907962\n",
      "Iteration  883 => Loss 33.874759\n",
      "Iteration  884 => Loss 33.841755\n",
      "Iteration  885 => Loss 33.808952\n",
      "Iteration  886 => Loss 33.776348\n",
      "Iteration  887 => Loss 33.743945\n",
      "Iteration  888 => Loss 33.711741\n",
      "Iteration  889 => Loss 33.679738\n",
      "Iteration  890 => Loss 33.647934\n",
      "Iteration  891 => Loss 33.616331\n",
      "Iteration  892 => Loss 33.584928\n",
      "Iteration  893 => Loss 33.553724\n",
      "Iteration  894 => Loss 33.522721\n",
      "Iteration  895 => Loss 33.491917\n",
      "Iteration  896 => Loss 33.461314\n",
      "Iteration  897 => Loss 33.425962\n",
      "Iteration  898 => Loss 33.393172\n",
      "Iteration  899 => Loss 33.360583\n",
      "Iteration  900 => Loss 33.328193\n",
      "Iteration  901 => Loss 33.296003\n",
      "Iteration  902 => Loss 33.264014\n",
      "Iteration  903 => Loss 33.232224\n",
      "Iteration  904 => Loss 33.200634\n",
      "Iteration  905 => Loss 33.169245\n",
      "Iteration  906 => Loss 33.138055\n",
      "Iteration  907 => Loss 33.107066\n",
      "Iteration  908 => Loss 33.076276\n",
      "Iteration  909 => Loss 33.045686\n",
      "Iteration  910 => Loss 33.015297\n",
      "Iteration  911 => Loss 32.985107\n",
      "Iteration  912 => Loss 32.955117\n",
      "Iteration  913 => Loss 32.925328\n",
      "Iteration  914 => Loss 32.891686\n",
      "Iteration  915 => Loss 32.859710\n",
      "Iteration  916 => Loss 32.827934\n",
      "Iteration  917 => Loss 32.796359\n",
      "Iteration  918 => Loss 32.764983\n",
      "Iteration  919 => Loss 32.733807\n",
      "Iteration  920 => Loss 32.702831\n",
      "Iteration  921 => Loss 32.672055\n",
      "Iteration  922 => Loss 32.641479\n",
      "Iteration  923 => Loss 32.611103\n",
      "Iteration  924 => Loss 32.580928\n",
      "Iteration  925 => Loss 32.550952\n",
      "Iteration  926 => Loss 32.521176\n",
      "Iteration  927 => Loss 32.491600\n",
      "Iteration  928 => Loss 32.462224\n",
      "Iteration  929 => Loss 32.433048\n",
      "Iteration  930 => Loss 32.404072\n",
      "Iteration  931 => Loss 32.372141\n",
      "Iteration  932 => Loss 32.340979\n",
      "Iteration  933 => Loss 32.310017\n",
      "Iteration  934 => Loss 32.279255\n",
      "Iteration  935 => Loss 32.248693\n",
      "Iteration  936 => Loss 32.218331\n",
      "Iteration  937 => Loss 32.188169\n",
      "Iteration  938 => Loss 32.158207\n",
      "Iteration  939 => Loss 32.128445\n",
      "Iteration  940 => Loss 32.098883\n",
      "Iteration  941 => Loss 32.069521\n",
      "Iteration  942 => Loss 32.040359\n",
      "Iteration  943 => Loss 32.011397\n",
      "Iteration  944 => Loss 31.982634\n",
      "Iteration  945 => Loss 31.954072\n",
      "Iteration  946 => Loss 31.925710\n",
      "Iteration  947 => Loss 31.897548\n",
      "Iteration  948 => Loss 31.869586\n",
      "Iteration  949 => Loss 31.836979\n",
      "Iteration  950 => Loss 31.806831\n",
      "Iteration  951 => Loss 31.776883\n",
      "Iteration  952 => Loss 31.747134\n",
      "Iteration  953 => Loss 31.717586\n",
      "Iteration  954 => Loss 31.688238\n",
      "Iteration  955 => Loss 31.659090\n",
      "Iteration  956 => Loss 31.630141\n",
      "Iteration  957 => Loss 31.601393\n",
      "Iteration  958 => Loss 31.572845\n",
      "Iteration  959 => Loss 31.544497\n",
      "Iteration  960 => Loss 31.516348\n",
      "Iteration  961 => Loss 31.488400\n",
      "Iteration  962 => Loss 31.460652\n",
      "Iteration  963 => Loss 31.433103\n",
      "Iteration  964 => Loss 31.405755\n",
      "Iteration  965 => Loss 31.378607\n",
      "Iteration  966 => Loss 31.347710\n",
      "Iteration  967 => Loss 31.318376\n",
      "Iteration  968 => Loss 31.289241\n",
      "Iteration  969 => Loss 31.260307\n",
      "Iteration  970 => Loss 31.231572\n",
      "Iteration  971 => Loss 31.203038\n",
      "Iteration  972 => Loss 31.174703\n",
      "Iteration  973 => Loss 31.146569\n",
      "Iteration  974 => Loss 31.118634\n",
      "Iteration  975 => Loss 31.090900\n",
      "Iteration  976 => Loss 31.063366\n",
      "Iteration  977 => Loss 31.036031\n",
      "Iteration  978 => Loss 31.008897\n",
      "Iteration  979 => Loss 30.981962\n",
      "Iteration  980 => Loss 30.955228\n",
      "Iteration  981 => Loss 30.928693\n",
      "Iteration  982 => Loss 30.902359\n",
      "Iteration  983 => Loss 30.873172\n",
      "Iteration  984 => Loss 30.844652\n",
      "Iteration  985 => Loss 30.816331\n",
      "Iteration  986 => Loss 30.788210\n",
      "Iteration  987 => Loss 30.760290\n",
      "Iteration  988 => Loss 30.732569\n",
      "Iteration  989 => Loss 30.705048\n",
      "Iteration  990 => Loss 30.677728\n",
      "Iteration  991 => Loss 30.650607\n",
      "Iteration  992 => Loss 30.623686\n",
      "Iteration  993 => Loss 30.596966\n",
      "Iteration  994 => Loss 30.570445\n",
      "Iteration  995 => Loss 30.544124\n",
      "Iteration  996 => Loss 30.518003\n",
      "Iteration  997 => Loss 30.492083\n",
      "Iteration  998 => Loss 30.466362\n",
      "Iteration  999 => Loss 30.440841\n",
      "Iteration 1000 => Loss 30.415521\n",
      "Iteration 1001 => Loss 30.385659\n",
      "Iteration 1002 => Loss 30.358152\n",
      "Iteration 1003 => Loss 30.330845\n",
      "Iteration 1004 => Loss 30.303738\n",
      "Iteration 1005 => Loss 30.276831\n",
      "Iteration 1006 => Loss 30.250124\n",
      "Iteration 1007 => Loss 30.223617\n",
      "Iteration 1008 => Loss 30.197310\n",
      "Iteration 1009 => Loss 30.171203\n",
      "Iteration 1010 => Loss 30.145297\n",
      "Iteration 1011 => Loss 30.119590\n",
      "Iteration 1012 => Loss 30.094083\n",
      "Iteration 1013 => Loss 30.068776\n",
      "Iteration 1014 => Loss 30.043669\n",
      "Iteration 1015 => Loss 30.018762\n",
      "Iteration 1016 => Loss 29.994055\n",
      "Iteration 1017 => Loss 29.969548\n",
      "Iteration 1018 => Loss 29.941397\n",
      "Iteration 1019 => Loss 29.914703\n",
      "Iteration 1020 => Loss 29.888210\n",
      "Iteration 1021 => Loss 29.861917\n",
      "Iteration 1022 => Loss 29.835824\n",
      "Iteration 1023 => Loss 29.809931\n",
      "Iteration 1024 => Loss 29.784238\n",
      "Iteration 1025 => Loss 29.758745\n",
      "Iteration 1026 => Loss 29.733452\n",
      "Iteration 1027 => Loss 29.708359\n",
      "Iteration 1028 => Loss 29.683466\n",
      "Iteration 1029 => Loss 29.658772\n",
      "Iteration 1030 => Loss 29.634279\n",
      "Iteration 1031 => Loss 29.609986\n",
      "Iteration 1032 => Loss 29.585893\n",
      "Iteration 1033 => Loss 29.562000\n",
      "Iteration 1034 => Loss 29.538307\n",
      "Iteration 1035 => Loss 29.511866\n",
      "Iteration 1036 => Loss 29.485986\n",
      "Iteration 1037 => Loss 29.460307\n",
      "Iteration 1038 => Loss 29.434828\n",
      "Iteration 1039 => Loss 29.409548\n",
      "Iteration 1040 => Loss 29.384469\n",
      "Iteration 1041 => Loss 29.359590\n",
      "Iteration 1042 => Loss 29.334910\n",
      "Iteration 1043 => Loss 29.310431\n",
      "Iteration 1044 => Loss 29.286152\n",
      "Iteration 1045 => Loss 29.262072\n",
      "Iteration 1046 => Loss 29.238193\n",
      "Iteration 1047 => Loss 29.214514\n",
      "Iteration 1048 => Loss 29.191034\n",
      "Iteration 1049 => Loss 29.167755\n",
      "Iteration 1050 => Loss 29.144676\n",
      "Iteration 1051 => Loss 29.121797\n",
      "Iteration 1052 => Loss 29.099117\n",
      "Iteration 1053 => Loss 29.072000\n",
      "Iteration 1054 => Loss 29.047134\n",
      "Iteration 1055 => Loss 29.022469\n",
      "Iteration 1056 => Loss 28.998003\n",
      "Iteration 1057 => Loss 28.973738\n",
      "Iteration 1058 => Loss 28.949672\n",
      "Iteration 1059 => Loss 28.925807\n",
      "Iteration 1060 => Loss 28.902141\n",
      "Iteration 1061 => Loss 28.878676\n",
      "Iteration 1062 => Loss 28.855410\n",
      "Iteration 1063 => Loss 28.832345\n",
      "Iteration 1064 => Loss 28.809479\n",
      "Iteration 1065 => Loss 28.786814\n",
      "Iteration 1066 => Loss 28.764348\n",
      "Iteration 1067 => Loss 28.742083\n",
      "Iteration 1068 => Loss 28.720017\n",
      "Iteration 1069 => Loss 28.698152\n",
      "Iteration 1070 => Loss 28.672745\n",
      "Iteration 1071 => Loss 28.648693\n",
      "Iteration 1072 => Loss 28.624841\n",
      "Iteration 1073 => Loss 28.601190\n",
      "Iteration 1074 => Loss 28.577738\n",
      "Iteration 1075 => Loss 28.554486\n",
      "Iteration 1076 => Loss 28.531434\n",
      "Iteration 1077 => Loss 28.508583\n",
      "Iteration 1078 => Loss 28.485931\n",
      "Iteration 1079 => Loss 28.463479\n",
      "Iteration 1080 => Loss 28.441228\n",
      "Iteration 1081 => Loss 28.419176\n",
      "Iteration 1082 => Loss 28.397324\n",
      "Iteration 1083 => Loss 28.375672\n",
      "Iteration 1084 => Loss 28.354221\n",
      "Iteration 1085 => Loss 28.332969\n",
      "Iteration 1086 => Loss 28.311917\n",
      "Iteration 1087 => Loss 28.288221\n",
      "Iteration 1088 => Loss 28.264983\n",
      "Iteration 1089 => Loss 28.241945\n",
      "Iteration 1090 => Loss 28.219107\n",
      "Iteration 1091 => Loss 28.196469\n",
      "Iteration 1092 => Loss 28.174031\n",
      "Iteration 1093 => Loss 28.151793\n",
      "Iteration 1094 => Loss 28.129755\n",
      "Iteration 1095 => Loss 28.107917\n",
      "Iteration 1096 => Loss 28.086279\n",
      "Iteration 1097 => Loss 28.064841\n",
      "Iteration 1098 => Loss 28.043603\n",
      "Iteration 1099 => Loss 28.022566\n",
      "Iteration 1100 => Loss 28.001728\n",
      "Iteration 1101 => Loss 27.981090\n",
      "Iteration 1102 => Loss 27.960652\n",
      "Iteration 1103 => Loss 27.940414\n",
      "Iteration 1104 => Loss 27.920376\n",
      "Iteration 1105 => Loss 27.896003\n",
      "Iteration 1106 => Loss 27.873779\n",
      "Iteration 1107 => Loss 27.851755\n",
      "Iteration 1108 => Loss 27.829931\n",
      "Iteration 1109 => Loss 27.808307\n",
      "Iteration 1110 => Loss 27.786883\n",
      "Iteration 1111 => Loss 27.765659\n",
      "Iteration 1112 => Loss 27.744634\n",
      "Iteration 1113 => Loss 27.723810\n",
      "Iteration 1114 => Loss 27.703186\n",
      "Iteration 1115 => Loss 27.682762\n",
      "Iteration 1116 => Loss 27.662538\n",
      "Iteration 1117 => Loss 27.642514\n",
      "Iteration 1118 => Loss 27.622690\n",
      "Iteration 1119 => Loss 27.603066\n",
      "Iteration 1120 => Loss 27.583641\n",
      "Iteration 1121 => Loss 27.564417\n",
      "Iteration 1122 => Loss 27.541755\n",
      "Iteration 1123 => Loss 27.520345\n",
      "Iteration 1124 => Loss 27.499134\n",
      "Iteration 1125 => Loss 27.478124\n",
      "Iteration 1126 => Loss 27.457314\n",
      "Iteration 1127 => Loss 27.436703\n",
      "Iteration 1128 => Loss 27.416293\n",
      "Iteration 1129 => Loss 27.396083\n",
      "Iteration 1130 => Loss 27.376072\n",
      "Iteration 1131 => Loss 27.356262\n",
      "Iteration 1132 => Loss 27.336652\n",
      "Iteration 1133 => Loss 27.317241\n",
      "Iteration 1134 => Loss 27.298031\n",
      "Iteration 1135 => Loss 27.279021\n",
      "Iteration 1136 => Loss 27.260210\n",
      "Iteration 1137 => Loss 27.241600\n",
      "Iteration 1138 => Loss 27.223190\n",
      "Iteration 1139 => Loss 27.202238\n",
      "Iteration 1140 => Loss 27.181641\n",
      "Iteration 1141 => Loss 27.161245\n",
      "Iteration 1142 => Loss 27.141048\n",
      "Iteration 1143 => Loss 27.121052\n",
      "Iteration 1144 => Loss 27.101255\n",
      "Iteration 1145 => Loss 27.081659\n",
      "Iteration 1146 => Loss 27.062262\n",
      "Iteration 1147 => Loss 27.043066\n",
      "Iteration 1148 => Loss 27.024069\n",
      "Iteration 1149 => Loss 27.005272\n",
      "Iteration 1150 => Loss 26.986676\n",
      "Iteration 1151 => Loss 26.968279\n",
      "Iteration 1152 => Loss 26.950083\n",
      "Iteration 1153 => Loss 26.932086\n",
      "Iteration 1154 => Loss 26.914290\n",
      "Iteration 1155 => Loss 26.896693\n",
      "Iteration 1156 => Loss 26.879297\n",
      "Iteration 1157 => Loss 26.857669\n",
      "Iteration 1158 => Loss 26.838086\n",
      "Iteration 1159 => Loss 26.818703\n",
      "Iteration 1160 => Loss 26.799521\n",
      "Iteration 1161 => Loss 26.780538\n",
      "Iteration 1162 => Loss 26.761755\n",
      "Iteration 1163 => Loss 26.743172\n",
      "Iteration 1164 => Loss 26.724790\n",
      "Iteration 1165 => Loss 26.706607\n",
      "Iteration 1166 => Loss 26.688624\n",
      "Iteration 1167 => Loss 26.670841\n",
      "Iteration 1168 => Loss 26.653259\n",
      "Iteration 1169 => Loss 26.635876\n",
      "Iteration 1170 => Loss 26.618693\n",
      "Iteration 1171 => Loss 26.601710\n",
      "Iteration 1172 => Loss 26.584928\n",
      "Iteration 1173 => Loss 26.568345\n",
      "Iteration 1174 => Loss 26.548428\n",
      "Iteration 1175 => Loss 26.529659\n",
      "Iteration 1176 => Loss 26.511090\n",
      "Iteration 1177 => Loss 26.492721\n",
      "Iteration 1178 => Loss 26.474552\n",
      "Iteration 1179 => Loss 26.456583\n",
      "Iteration 1180 => Loss 26.438814\n",
      "Iteration 1181 => Loss 26.421245\n",
      "Iteration 1182 => Loss 26.403876\n",
      "Iteration 1183 => Loss 26.386707\n",
      "Iteration 1184 => Loss 26.369738\n",
      "Iteration 1185 => Loss 26.352969\n",
      "Iteration 1186 => Loss 26.336400\n",
      "Iteration 1187 => Loss 26.320031\n",
      "Iteration 1188 => Loss 26.303862\n",
      "Iteration 1189 => Loss 26.287893\n",
      "Iteration 1190 => Loss 26.272124\n",
      "Iteration 1191 => Loss 26.253917\n",
      "Iteration 1192 => Loss 26.235962\n",
      "Iteration 1193 => Loss 26.218207\n",
      "Iteration 1194 => Loss 26.200652\n",
      "Iteration 1195 => Loss 26.183297\n",
      "Iteration 1196 => Loss 26.166141\n",
      "Iteration 1197 => Loss 26.149186\n",
      "Iteration 1198 => Loss 26.132431\n",
      "Iteration 1199 => Loss 26.115876\n",
      "Iteration 1200 => Loss 26.099521\n",
      "Iteration 1201 => Loss 26.083366\n",
      "Iteration 1202 => Loss 26.067410\n",
      "Iteration 1203 => Loss 26.051655\n",
      "Iteration 1204 => Loss 26.036100\n",
      "Iteration 1205 => Loss 26.020745\n",
      "Iteration 1206 => Loss 26.005590\n",
      "Iteration 1207 => Loss 25.990634\n",
      "Iteration 1208 => Loss 25.975879\n",
      "Iteration 1209 => Loss 25.956997\n",
      "Iteration 1210 => Loss 25.940055\n",
      "Iteration 1211 => Loss 25.923314\n",
      "Iteration 1212 => Loss 25.906772\n",
      "Iteration 1213 => Loss 25.890431\n",
      "Iteration 1214 => Loss 25.874290\n",
      "Iteration 1215 => Loss 25.858348\n",
      "Iteration 1216 => Loss 25.842607\n",
      "Iteration 1217 => Loss 25.827066\n",
      "Iteration 1218 => Loss 25.811724\n",
      "Iteration 1219 => Loss 25.796583\n",
      "Iteration 1220 => Loss 25.781641\n",
      "Iteration 1221 => Loss 25.766900\n",
      "Iteration 1222 => Loss 25.752359\n",
      "Iteration 1223 => Loss 25.738017\n",
      "Iteration 1224 => Loss 25.723876\n",
      "Iteration 1225 => Loss 25.709934\n",
      "Iteration 1226 => Loss 25.692762\n",
      "Iteration 1227 => Loss 25.676634\n",
      "Iteration 1228 => Loss 25.660707\n",
      "Iteration 1229 => Loss 25.644979\n",
      "Iteration 1230 => Loss 25.629452\n",
      "Iteration 1231 => Loss 25.614124\n",
      "Iteration 1232 => Loss 25.598997\n",
      "Iteration 1233 => Loss 25.584069\n",
      "Iteration 1234 => Loss 25.569341\n",
      "Iteration 1235 => Loss 25.554814\n",
      "Iteration 1236 => Loss 25.540486\n",
      "Iteration 1237 => Loss 25.526359\n",
      "Iteration 1238 => Loss 25.512431\n",
      "Iteration 1239 => Loss 25.498703\n",
      "Iteration 1240 => Loss 25.485176\n",
      "Iteration 1241 => Loss 25.471848\n",
      "Iteration 1242 => Loss 25.458721\n",
      "Iteration 1243 => Loss 25.445793\n",
      "Iteration 1244 => Loss 25.427945\n",
      "Iteration 1245 => Loss 25.412831\n",
      "Iteration 1246 => Loss 25.397917\n",
      "Iteration 1247 => Loss 25.383203\n",
      "Iteration 1248 => Loss 25.368690\n",
      "Iteration 1249 => Loss 25.354376\n",
      "Iteration 1250 => Loss 25.340262\n",
      "Iteration 1251 => Loss 25.326348\n",
      "Iteration 1252 => Loss 25.312634\n",
      "Iteration 1253 => Loss 25.299121\n",
      "Iteration 1254 => Loss 25.285807\n",
      "Iteration 1255 => Loss 25.272693\n",
      "Iteration 1256 => Loss 25.259779\n",
      "Iteration 1257 => Loss 25.247066\n",
      "Iteration 1258 => Loss 25.234552\n",
      "Iteration 1259 => Loss 25.222238\n",
      "Iteration 1260 => Loss 25.210124\n",
      "Iteration 1261 => Loss 25.193986\n",
      "Iteration 1262 => Loss 25.179686\n",
      "Iteration 1263 => Loss 25.165586\n",
      "Iteration 1264 => Loss 25.151686\n",
      "Iteration 1265 => Loss 25.137986\n",
      "Iteration 1266 => Loss 25.124486\n",
      "Iteration 1267 => Loss 25.111186\n",
      "Iteration 1268 => Loss 25.098086\n",
      "Iteration 1269 => Loss 25.085186\n",
      "Iteration 1270 => Loss 25.072486\n",
      "Iteration 1271 => Loss 25.059986\n",
      "Iteration 1272 => Loss 25.047686\n",
      "Iteration 1273 => Loss 25.035586\n",
      "Iteration 1274 => Loss 25.023686\n",
      "Iteration 1275 => Loss 25.011986\n",
      "Iteration 1276 => Loss 25.000486\n",
      "Iteration 1277 => Loss 24.989186\n",
      "Iteration 1278 => Loss 24.974759\n",
      "Iteration 1279 => Loss 24.961272\n",
      "Iteration 1280 => Loss 24.947986\n",
      "Iteration 1281 => Loss 24.934900\n",
      "Iteration 1282 => Loss 24.922014\n",
      "Iteration 1283 => Loss 24.909328\n",
      "Iteration 1284 => Loss 24.896841\n",
      "Iteration 1285 => Loss 24.884555\n",
      "Iteration 1286 => Loss 24.872469\n",
      "Iteration 1287 => Loss 24.860583\n",
      "Iteration 1288 => Loss 24.848897\n",
      "Iteration 1289 => Loss 24.837410\n",
      "Iteration 1290 => Loss 24.826124\n",
      "Iteration 1291 => Loss 24.815038\n",
      "Iteration 1292 => Loss 24.804152\n",
      "Iteration 1293 => Loss 24.793466\n",
      "Iteration 1294 => Loss 24.782979\n",
      "Iteration 1295 => Loss 24.772693\n",
      "Iteration 1296 => Loss 24.757590\n",
      "Iteration 1297 => Loss 24.745117\n",
      "Iteration 1298 => Loss 24.732845\n",
      "Iteration 1299 => Loss 24.720772\n",
      "Iteration 1300 => Loss 24.708900\n",
      "Iteration 1301 => Loss 24.697228\n",
      "Iteration 1302 => Loss 24.685755\n",
      "Iteration 1303 => Loss 24.674483\n",
      "Iteration 1304 => Loss 24.663410\n",
      "Iteration 1305 => Loss 24.652538\n",
      "Iteration 1306 => Loss 24.641866\n",
      "Iteration 1307 => Loss 24.631393\n",
      "Iteration 1308 => Loss 24.621121\n",
      "Iteration 1309 => Loss 24.611048\n",
      "Iteration 1310 => Loss 24.601176\n",
      "Iteration 1311 => Loss 24.591503\n",
      "Iteration 1312 => Loss 24.582031\n",
      "Iteration 1313 => Loss 24.568638\n",
      "Iteration 1314 => Loss 24.556979\n",
      "Iteration 1315 => Loss 24.545521\n",
      "Iteration 1316 => Loss 24.534262\n",
      "Iteration 1317 => Loss 24.523203\n",
      "Iteration 1318 => Loss 24.512345\n",
      "Iteration 1319 => Loss 24.501686\n",
      "Iteration 1320 => Loss 24.491228\n",
      "Iteration 1321 => Loss 24.480969\n",
      "Iteration 1322 => Loss 24.470910\n",
      "Iteration 1323 => Loss 24.461052\n",
      "Iteration 1324 => Loss 24.451393\n",
      "Iteration 1325 => Loss 24.441934\n",
      "Iteration 1326 => Loss 24.432676\n",
      "Iteration 1327 => Loss 24.423617\n",
      "Iteration 1328 => Loss 24.414759\n",
      "Iteration 1329 => Loss 24.406100\n",
      "Iteration 1330 => Loss 24.394417\n",
      "Iteration 1331 => Loss 24.383572\n",
      "Iteration 1332 => Loss 24.372928\n",
      "Iteration 1333 => Loss 24.362483\n",
      "Iteration 1334 => Loss 24.352238\n",
      "Iteration 1335 => Loss 24.342193\n",
      "Iteration 1336 => Loss 24.332348\n",
      "Iteration 1337 => Loss 24.322703\n",
      "Iteration 1338 => Loss 24.313259\n",
      "Iteration 1339 => Loss 24.304014\n",
      "Iteration 1340 => Loss 24.294969\n",
      "Iteration 1341 => Loss 24.286124\n",
      "Iteration 1342 => Loss 24.277479\n",
      "Iteration 1343 => Loss 24.269034\n",
      "Iteration 1344 => Loss 24.260790\n",
      "Iteration 1345 => Loss 24.252745\n",
      "Iteration 1346 => Loss 24.244900\n",
      "Iteration 1347 => Loss 24.237255\n",
      "Iteration 1348 => Loss 24.224897\n",
      "Iteration 1349 => Loss 24.215066\n",
      "Iteration 1350 => Loss 24.205434\n",
      "Iteration 1351 => Loss 24.196003\n",
      "Iteration 1352 => Loss 24.186772\n",
      "Iteration 1353 => Loss 24.177741\n",
      "Iteration 1354 => Loss 24.168910\n",
      "Iteration 1355 => Loss 24.160279\n",
      "Iteration 1356 => Loss 24.151848\n",
      "Iteration 1357 => Loss 24.143617\n",
      "Iteration 1358 => Loss 24.135586\n",
      "Iteration 1359 => Loss 24.127755\n",
      "Iteration 1360 => Loss 24.120124\n",
      "Iteration 1361 => Loss 24.112693\n",
      "Iteration 1362 => Loss 24.105462\n",
      "Iteration 1363 => Loss 24.098431\n",
      "Iteration 1364 => Loss 24.091600\n",
      "Iteration 1365 => Loss 24.080952\n",
      "Iteration 1366 => Loss 24.071934\n",
      "Iteration 1367 => Loss 24.063117\n",
      "Iteration 1368 => Loss 24.054500\n",
      "Iteration 1369 => Loss 24.046083\n",
      "Iteration 1370 => Loss 24.037866\n",
      "Iteration 1371 => Loss 24.029848\n",
      "Iteration 1372 => Loss 24.022031\n",
      "Iteration 1373 => Loss 24.014414\n",
      "Iteration 1374 => Loss 24.006997\n",
      "Iteration 1375 => Loss 23.999779\n",
      "Iteration 1376 => Loss 23.992762\n",
      "Iteration 1377 => Loss 23.985945\n",
      "Iteration 1378 => Loss 23.979328\n",
      "Iteration 1379 => Loss 23.972910\n",
      "Iteration 1380 => Loss 23.966693\n",
      "Iteration 1381 => Loss 23.960676\n",
      "Iteration 1382 => Loss 23.951738\n",
      "Iteration 1383 => Loss 23.943534\n",
      "Iteration 1384 => Loss 23.935531\n",
      "Iteration 1385 => Loss 23.927728\n",
      "Iteration 1386 => Loss 23.920124\n",
      "Iteration 1387 => Loss 23.912721\n",
      "Iteration 1388 => Loss 23.905517\n",
      "Iteration 1389 => Loss 23.898514\n",
      "Iteration 1390 => Loss 23.891710\n",
      "Iteration 1391 => Loss 23.885107\n",
      "Iteration 1392 => Loss 23.878703\n",
      "Iteration 1393 => Loss 23.872500\n",
      "Iteration 1394 => Loss 23.866497\n",
      "Iteration 1395 => Loss 23.860693\n",
      "Iteration 1396 => Loss 23.855090\n",
      "Iteration 1397 => Loss 23.849686\n",
      "Iteration 1398 => Loss 23.844483\n",
      "Iteration 1399 => Loss 23.839479\n",
      "Iteration 1400 => Loss 23.829866\n",
      "Iteration 1401 => Loss 23.822676\n",
      "Iteration 1402 => Loss 23.815686\n",
      "Iteration 1403 => Loss 23.808897\n",
      "Iteration 1404 => Loss 23.802307\n",
      "Iteration 1405 => Loss 23.795917\n",
      "Iteration 1406 => Loss 23.789728\n",
      "Iteration 1407 => Loss 23.783738\n",
      "Iteration 1408 => Loss 23.777948\n",
      "Iteration 1409 => Loss 23.772359\n",
      "Iteration 1410 => Loss 23.766969\n",
      "Iteration 1411 => Loss 23.761779\n",
      "Iteration 1412 => Loss 23.756790\n",
      "Iteration 1413 => Loss 23.752000\n",
      "Iteration 1414 => Loss 23.747410\n",
      "Iteration 1415 => Loss 23.743021\n",
      "Iteration 1416 => Loss 23.738831\n",
      "Iteration 1417 => Loss 23.730928\n",
      "Iteration 1418 => Loss 23.724552\n",
      "Iteration 1419 => Loss 23.718376\n",
      "Iteration 1420 => Loss 23.712400\n",
      "Iteration 1421 => Loss 23.706624\n",
      "Iteration 1422 => Loss 23.701048\n",
      "Iteration 1423 => Loss 23.695672\n",
      "Iteration 1424 => Loss 23.690497\n",
      "Iteration 1425 => Loss 23.685521\n",
      "Iteration 1426 => Loss 23.680745\n",
      "Iteration 1427 => Loss 23.676169\n",
      "Iteration 1428 => Loss 23.671793\n",
      "Iteration 1429 => Loss 23.667617\n",
      "Iteration 1430 => Loss 23.663641\n",
      "Iteration 1431 => Loss 23.659866\n",
      "Iteration 1432 => Loss 23.656290\n",
      "Iteration 1433 => Loss 23.652914\n",
      "Iteration 1434 => Loss 23.646721\n",
      "Iteration 1435 => Loss 23.641159\n",
      "Iteration 1436 => Loss 23.635797\n",
      "Iteration 1437 => Loss 23.630634\n",
      "Iteration 1438 => Loss 23.625672\n",
      "Iteration 1439 => Loss 23.620910\n",
      "Iteration 1440 => Loss 23.616348\n",
      "Iteration 1441 => Loss 23.611986\n",
      "Iteration 1442 => Loss 23.607824\n",
      "Iteration 1443 => Loss 23.603862\n",
      "Iteration 1444 => Loss 23.600100\n",
      "Iteration 1445 => Loss 23.596538\n",
      "Iteration 1446 => Loss 23.593176\n",
      "Iteration 1447 => Loss 23.590014\n",
      "Iteration 1448 => Loss 23.587052\n",
      "Iteration 1449 => Loss 23.584290\n",
      "Iteration 1450 => Loss 23.581728\n",
      "Iteration 1451 => Loss 23.579366\n",
      "Iteration 1452 => Loss 23.572497\n",
      "Iteration 1453 => Loss 23.567948\n",
      "Iteration 1454 => Loss 23.563600\n",
      "Iteration 1455 => Loss 23.559452\n",
      "Iteration 1456 => Loss 23.555503\n",
      "Iteration 1457 => Loss 23.551755\n",
      "Iteration 1458 => Loss 23.548207\n",
      "Iteration 1459 => Loss 23.544859\n",
      "Iteration 1460 => Loss 23.541710\n",
      "Iteration 1461 => Loss 23.538762\n",
      "Iteration 1462 => Loss 23.536014\n",
      "Iteration 1463 => Loss 23.533466\n",
      "Iteration 1464 => Loss 23.531117\n",
      "Iteration 1465 => Loss 23.528969\n",
      "Iteration 1466 => Loss 23.527021\n",
      "Iteration 1467 => Loss 23.525272\n",
      "Iteration 1468 => Loss 23.523724\n",
      "Iteration 1469 => Loss 23.518566\n",
      "Iteration 1470 => Loss 23.514831\n",
      "Iteration 1471 => Loss 23.511297\n",
      "Iteration 1472 => Loss 23.507962\n",
      "Iteration 1473 => Loss 23.504828\n",
      "Iteration 1474 => Loss 23.501893\n",
      "Iteration 1475 => Loss 23.499159\n",
      "Iteration 1476 => Loss 23.496624\n",
      "Iteration 1477 => Loss 23.494290\n",
      "Iteration 1478 => Loss 23.492155\n",
      "Iteration 1479 => Loss 23.490221\n",
      "Iteration 1480 => Loss 23.488486\n",
      "Iteration 1481 => Loss 23.486952\n",
      "Iteration 1482 => Loss 23.485617\n",
      "Iteration 1483 => Loss 23.484483\n",
      "Iteration 1484 => Loss 23.483548\n",
      "Iteration 1485 => Loss 23.482814\n",
      "Iteration 1486 => Loss 23.479366\n",
      "Iteration 1487 => Loss 23.476445\n",
      "Iteration 1488 => Loss 23.473724\n",
      "Iteration 1489 => Loss 23.471203\n",
      "Iteration 1490 => Loss 23.468883\n",
      "Iteration 1491 => Loss 23.466762\n",
      "Iteration 1492 => Loss 23.464841\n",
      "Iteration 1493 => Loss 23.463121\n",
      "Iteration 1494 => Loss 23.461600\n",
      "Iteration 1495 => Loss 23.460279\n",
      "Iteration 1496 => Loss 23.459159\n",
      "Iteration 1497 => Loss 23.458238\n",
      "Iteration 1498 => Loss 23.457517\n",
      "Iteration 1499 => Loss 23.456997\n",
      "Iteration 1500 => Loss 23.456676\n",
      "Iteration 1501 => Loss 23.456555\n"
     ]
    }
   ],
   "source": [
    "w, b = train(reservations, pizzas, 10000, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the data with the linear modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Reservations', fontsize=16)\n",
    "plt.ylabel('Pizzas', fontsize=16)\n",
    "plt.axis([0,50,0,50])\n",
    "plt.plot(reservations, pizzas, \"o\")\n",
    "plt.plot([0, 50], [b, predict(50, w, b)], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.48999999999981"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservations = 59\n",
    "predict(reservations, w, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
